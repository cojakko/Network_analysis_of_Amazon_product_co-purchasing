{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadati caricati: (548552, 4)\n",
      "   Id        ASIN                                              title group\n",
      "0   0  0771044445                                                NaN   NaN\n",
      "1   1  0827229534            Patterns of Preaching: A Sermon Sampler  Book\n",
      "2   2  0738700797                         Candlemas: Feast of Flames  Book\n",
      "3   3  0486287785   World War II Allied Fighter Planes Trading Cards  Book\n",
      "4   4  0842328327  Life Application Bible Commentary: 1 and 2 Tim...  Book\n",
      "Archi caricati: (3356824, 2)\n",
      "   source  target\n",
      "0       0       1\n",
      "1       0       2\n",
      "2       0       3\n",
      "3       0       4\n",
      "4       0       5\n",
      "Nodi usati nel grafo: (410236, 4)\n",
      "nodes.csv e edges.csv creati correttamente!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. PARSE DEL FILE amazon-meta.txt PER ESTRARRE Id, title, group\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "meta_file = \"/Users/michelebaldo/Desktop/amazon-meta.txt\"\n",
    "\n",
    "records = []\n",
    "current = {}\n",
    "\n",
    "with open(meta_file, \"r\", encoding=\"latin1\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Nuovo record\n",
    "        if line.startswith(\"Id:\"):\n",
    "            # Salva il record precedente\n",
    "            if \"Id\" in current:\n",
    "                records.append(current)\n",
    "            current = {\"Id\": int(line.split()[1])}\n",
    "\n",
    "        # ASIN\n",
    "        elif line.startswith(\"ASIN:\"):\n",
    "            current[\"ASIN\"] = line.replace(\"ASIN:\", \"\").strip()\n",
    "\n",
    "        # title\n",
    "        elif line.startswith(\"title:\"):\n",
    "            current[\"title\"] = line.replace(\"title:\", \"\").strip()\n",
    "\n",
    "        # group\n",
    "        elif line.startswith(\"group:\"):\n",
    "            current[\"group\"] = line.replace(\"group:\", \"\").strip()\n",
    "\n",
    "# Aggiungi l’ultimo record\n",
    "if \"Id\" in current:\n",
    "    records.append(current)\n",
    "\n",
    "meta_df = pd.DataFrame(records)\n",
    "\n",
    "print(\"Metadati caricati:\", meta_df.shape)\n",
    "print(meta_df.head())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. CARICAMENTO ARCHI DA Amazon0505.csv\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "edges = pd.read_csv(\"/Users/michelebaldo/Desktop/Amazon0505.csv\", comment=\"#\", sep=\"\\t\",\n",
    "                    names=[\"source\", \"target\"])\n",
    "\n",
    "print(\"Archi caricati:\", edges.shape)\n",
    "print(edges.head())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. FILTRA I NODI PRESENTI NEGLI ARCHI\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "used_ids = set(edges[\"source\"]).union(set(edges[\"target\"]))\n",
    "meta_df = meta_df[meta_df[\"Id\"].isin(used_ids)]\n",
    "\n",
    "print(\"Nodi usati nel grafo:\", meta_df.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. PREPARA NODES.CSV PER GEPHI\n",
    "# id = Id\n",
    "# label = title (se manca → ASIN)\n",
    "# group = categoria\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "meta_df[\"label\"] = meta_df[\"title\"].fillna(meta_df[\"ASIN\"])\n",
    "nodes = meta_df[[\"Id\", \"label\", \"group\"]]\n",
    "nodes.columns = [\"id\", \"label\", \"group\"]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. PREPARA EDGES.CSV PER GEPHI\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "edges = edges[edges[\"source\"].isin(nodes[\"id\"]) &\n",
    "              edges[\"target\"].isin(nodes[\"id\"])]\n",
    "\n",
    "edges.columns = [\"source\", \"target\"]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. ESPORTA I FILE FINALI\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "nodes.to_csv(\"nodes2.csv\", index=False)\n",
    "edges.to_csv(\"edges2.csv\", index=False)\n",
    "\n",
    "print(\"nodes.csv e edges.csv creati correttamente!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulizia completata! Salvati nodes_clean.csv e edges_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i file\n",
    "nodes = pd.read_csv(\"/Users/michelebaldo/Desktop/nodes2.csv\")\n",
    "edges = pd.read_csv(\"/Users/michelebaldo/Desktop/edges2.csv\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Rimuovi il nodo con id = 0 da nodes.csv\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "nodes_clean = nodes[nodes[\"id\"] != 0]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Rimuovi tutti gli archi che coinvolgono il nodo 0\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "edges_clean = edges[(edges[\"source\"] != 0) & (edges[\"target\"] != 0)]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Salva i nuovi file puliti\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "nodes_clean.to_csv(\"nodes_clean.csv\", index=False)\n",
    "edges_clean.to_csv(\"edges_clean.csv\", index=False)\n",
    "\n",
    "print(\"Pulizia completata! Salvati nodes_clean.csv e edges_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creati correttamente: nodes_top100.csv e edges_top100.csv\n",
      "Nodi selezionati: 100\n",
      "Archi filtrati: 251\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Caricare i file originali puliti\n",
    "# -------------------------------------------------------------------\n",
    "nodes = pd.read_csv(\"/Users/michelebaldo/Desktop/nodes_clean.csv\")\n",
    "edges = pd.read_csv(\"/Users/michelebaldo/Desktop/edges_clean.csv\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Calcolare il degree totale (in-degree + out-degree)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Degree = quante volte un nodo compare come source o target\n",
    "out_degree = edges.groupby(\"source\").size().rename(\"out_degree\")\n",
    "in_degree = edges.groupby(\"target\").size().rename(\"in_degree\")\n",
    "\n",
    "degree = pd.concat([in_degree, out_degree], axis=1).fillna(0)\n",
    "degree[\"total_degree\"] = degree[\"in_degree\"] + degree[\"out_degree\"]\n",
    "\n",
    "# Unire il degree ai nodi\n",
    "nodes_deg = nodes.merge(degree, left_on=\"id\", right_index=True, how=\"left\")\n",
    "nodes_deg = nodes_deg.fillna(0)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Selezionare i top 100 nodi per degree\n",
    "# -------------------------------------------------------------------\n",
    "top100 = nodes_deg.sort_values(\"total_degree\", ascending=False).head(100)\n",
    "\n",
    "top100_ids = set(top100[\"id\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Filtrare gli archi che coinvolgono solo questi 100 nodi\n",
    "# -------------------------------------------------------------------\n",
    "edges_top100 = edges[\n",
    "    edges[\"source\"].isin(top100_ids) & edges[\"target\"].isin(top100_ids)\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Esportare i file pronti per Gephi\n",
    "# -------------------------------------------------------------------\n",
    "top100.to_csv(\"nodes_top100.csv\", index=False)\n",
    "edges_top100.to_csv(\"edges_top100.csv\", index=False)\n",
    "\n",
    "print(\"Creati correttamente: nodes_top100.csv e edges_top100.csv\")\n",
    "print(\"Nodi selezionati:\", len(top100_ids))\n",
    "print(\"Archi filtrati:\", len(edges_top100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ordinato creato: nodes_top100_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il file che vuoi ordinare\n",
    "nodes = pd.read_csv(\"/Users/michelebaldo/Desktop/nodes_top100.csv\")   # <-- cambia il nome se serve\n",
    "\n",
    "# Ordina per id crescente\n",
    "nodes_sorted = nodes.sort_values(by=\"id\", ascending=True)\n",
    "\n",
    "# Salva il file ordinato\n",
    "nodes_sorted.to_csv(\"nodes_top100_sorted.csv\", index=False)\n",
    "\n",
    "print(\"File ordinato creato: nodes_top100_sorted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File edges ordinato creato: edges_top100_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "edges = pd.read_csv(\"/Users/michelebaldo/Desktop/edges_top100.csv\")\n",
    "\n",
    "edges_sorted = edges.sort_values(by=[\"source\", \"target\"], ascending=True)\n",
    "\n",
    "edges_sorted.to_csv(\"edges_top100_sorted.csv\", index=False)\n",
    "\n",
    "print(\"File edges ordinato creato: edges_top100_sorted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato edges_top100_weighted.csv con colonna 'weight'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caricamento dei file top100\n",
    "nodes = pd.read_csv(\"/Users/michelebaldo/Desktop/nodes_top100.csv\")\n",
    "edges = pd.read_csv(\"/Users/michelebaldo/Desktop/edges_top100.csv\")\n",
    "\n",
    "# Creiamo un dizionario {id : total_degree}\n",
    "degree_map = dict(zip(nodes[\"id\"], nodes[\"total_degree\"]))\n",
    "\n",
    "# Calcoliamo il peso artificiale dell'arco\n",
    "edges[\"weight\"] = edges.apply(\n",
    "    lambda row: degree_map[row[\"source\"]] + degree_map[row[\"target\"]],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Salviamo il nuovo file\n",
    "edges.to_csv(\"edges_top100_weighted.csv\", index=False)\n",
    "\n",
    "print(\"Creato edges_top100_weighted.csv con colonna 'weight'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
